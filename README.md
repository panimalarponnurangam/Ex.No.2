
# Ex.No: 2 	Evaluation of Prompting Tools Across Diverse AI Platforms: ChatGPT, Claude, Bard, Cohere Command, and Meta 
### DATE:                                                                            
### REGISTER NUMBER : 212222110031
### NAME : PANIMALAR P
 
### Aim:
To compare the performance, user experience, and response quality of different AI platforms (ChatGPT, Claude, Bard, Cohere Command, and Meta) within a specific use case, such as summarizing text or answering technical questions. Generate a Prompt based output using different Prompting.
### AI Tools required:
- ChatGPT (GPT-4) ->	chat.openai.com
- Claude	claude.ai -> (Anthropic)
- Bard (Gemini) ->	bard.google.com
- Cohere Command R+ ->	cohere.com API access
- Meta LLaMA ->	Open-source via HuggingFace or local deployment

### Methodology
# Platforms Tested: 
ChatGPT (OpenAI GPT-4), Claude (Anthropic), Bard (Google Gemini), Cohere Command R+, Meta LLaMA (through open-source deployment or third-party interfaces).

# Prompt Types:
Informative, Creative, Logical Reasoning, Instruction-following, and Conversational.

# Evaluation Metrics:
- Relevance (How closely the output matches the prompt)
- Accuracy (Correctness of information)
- Creativity (Novelty in responses)
- Conciseness (Clear and brief)
- Tone & Coherence (Natural language quality)

# Procedure:
1.Standardized prompts were created across 5 types.
2.Each was entered into all 5 AI platforms.
3.Responses were collected and evaluated by 3 human reviewers using a 1‚Äì5 rating scale for each metric.
4.Averages were calculated and compared across models.

### Prompt Types Explained:

## 1. Informative Prompt
**Prompt:** ‚ÄúExplain Quantum Computing in simple terms.‚Äù

**ChatGPT:** Clear analogy-based explanation using real-world comparisons.
**Claude:** Similar clarity but slightly more verbose.
**Bard:** Balanced technical depth and simplicity.
**Cohere Command:** Less engaging, but accurate.
**Meta LLaMA:** Technical, but less user-friendly.

## 2. Creative Prompt
**Prompt:** ‚ÄúWrite a poem about time as a river.‚Äù

**ChatGPT:** Vivid imagery, metaphor-rich.
**Claude:** Philosophical tone with narrative flow.
**Bard:** Elegant but slightly mechanical.
**Cohere Command:** Limited creativity.
**Meta LLaMA:** Abstract, inconsistent rhyme.

## 3. Logical Reasoning Prompt
**Prompt:** ‚ÄúIf all cats are mammals and some mammals are not cats, is it true that all mammals are cats?‚Äù

**ChatGPT:** Clear logical reasoning, correct.
**Claude:** Correct with step-by-step reasoning.
**Bard:** Accurate but less detailed.
**Cohere Command:** Confused logic.
**Meta LLaMA:** Inconsistent logic.

4. üõ†Ô∏è Instruction-following Prompt
**Prompt:** ‚ÄúList 3 benefits of solar energy in bullet points.‚Äù

**ChatGPT:** Follows instruction perfectly.

**Claude:** Follows well with extra explanation.

**Bard:** Sometimes adds more than 3.

**Cohere Command:** Sometimes misses bullet formatting.

Meta LLaMA: May skip structure or add extras.

5. üó£Ô∏è Conversational Prompt
Prompt: ‚ÄúI‚Äôm feeling nervous before an exam. Can you help?‚Äù

ChatGPT: Empathetic, practical advice.

Claude: Gentle and supportive.

Bard: Informative but less emotional.

Cohere Command: Robotic tone.

Meta LLaMA: Not emotionally aligned.
7. üíª Code Generation Prompt (New)
Prompt: ‚ÄúWrite a Python function to check if a string is a palindrome.‚Äù

ChatGPT: Correct, well-commented code

Claude: Accurate with explanations

Bard: Clean code, minimal comments

Cohere: Works but no explanation

Meta: Often incomplete or syntax errors


### Conclusion: 


# Result : The Prompt for the above problem statement executed successfully.
